{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAN Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kan\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from kan import KAN, create_dataset, KANLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mnist data\n",
    "from torchvision import datasets, transforms\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classical VAE\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = torch.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function (ELBO)\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 54948.285156\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 18571.414062\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 15855.440430\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 14476.313477\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 13529.828125\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 12869.540039\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 12602.191406\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 12019.205078\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 12015.060547\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 12341.388672\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 11288.124023\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 11724.761719\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 11607.344727\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 11374.126953\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 11090.015625\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 11614.677734\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 11064.367188\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 11422.479492\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 11183.132812\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 10951.351562\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 11115.054688\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 11037.121094\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 10614.533203\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 10700.044922\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 10956.364258\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 11015.732422\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 11071.078125\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 10686.154297\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 10675.847656\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 10811.005859\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 11200.831055\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 10869.714844\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 10919.554688\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 10803.888672\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 10516.217773\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 10597.838867\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 11139.245117\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 10924.588867\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 10666.363281\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 10946.842773\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 10880.322266\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 10554.438477\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 10916.344727\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 10597.208008\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 10414.516602\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 10917.904297\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 10376.125000\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 10622.460938\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 10421.105469\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 10371.903320\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 10622.761719\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 10603.586914\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 10669.546875\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 11039.734375\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 10700.102539\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 10967.552734\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 10529.673828\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 10271.523438\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 10446.530273\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 10750.542969\n"
     ]
    }
   ],
   "source": [
    "# Train the VAE\n",
    "vae = VAE()\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "vae.train()\n",
    "losses = [] \n",
    "for epoch in range(10):\n",
    "    for i, (data, _) in enumerate(train_loader):\n",
    "        data = data.view(-1, 784)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        if i % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(data), len(train_loader.dataset),\n",
    "                100. * i / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set loss: 105.3020\n"
     ]
    }
   ],
   "source": [
    "# Test the VAE\n",
    "vae.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        data = data.view(-1, 784)\n",
    "        recon_batch, mu, logvar = vae(data)\n",
    "        test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Training loss plot\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mlosses\u001b[49m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "# Training loss plot\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAN-VAE\n",
    "\n",
    "class KANVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KANVAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
