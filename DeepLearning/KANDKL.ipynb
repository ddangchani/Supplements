{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Kernel Learning with KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from kan import KANLayer, KAN, LBFGS\n",
    "import gpytorch\n",
    "import math\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "try:\n",
    "    chdir_bool\n",
    "except NameError:\n",
    "    os.chdir(os.getcwd() + \"/..\")\n",
    "    chdir_bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13279, 18]) torch.Size([3320, 18]) torch.Size([13279]) torch.Size([3320])\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "data = torch.Tensor(loadmat(\"data/elevators.mat\")[\"data\"])\n",
    "\n",
    "X = data[:, :-1]\n",
    "X -= X.mean(dim=0)\n",
    "X = 2 * (X / X.max(dim=0, keepdim=True).values) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DKL\n",
    "data_dim = X_train.shape[1]\n",
    "\n",
    "class FeatureExtractor(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', nn.Linear(data_dim, 100))\n",
    "        self.add_module('relu1', nn.ReLU())\n",
    "        self.add_module('linear2', nn.Linear(100, 50))\n",
    "        self.add_module('relu2', nn.ReLU())\n",
    "        self.add_module('linear3', nn.Linear(50, 10))\n",
    "        self.add_module('relu3', nn.ReLU())\n",
    "        self.add_module('linear4', nn.Linear(10, 2))\n",
    "\n",
    "feature_extractor = FeatureExtractor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPR(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPR, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=2)),\n",
    "            num_dims=2, grid_size=100\n",
    "        )\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.scaler = gpytorch.utils.grid.ScaleToBounds(-1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.scaler(x)\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPR(X_train, y_train, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 7487\n"
     ]
    }
   ],
   "source": [
    "# number of parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {n_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62574b84f824ccebf58572062a15291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Loss\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "def train():\n",
    "    history = {'loss': []}  # Create a dictionary to store the history\n",
    "    iterator = tqdm(range(epochs), desc=\"Epochs\")\n",
    "    for epoch in iterator:\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(X_train)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        history['loss'].append(loss.item())\n",
    "        iterator.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "    return history\n",
    "\n",
    "hist = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.09314639121294022\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Plotting\u001b[39;00m\n\u001b[1;32m     11\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m---> 12\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhist\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     13\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    preds = model(X_test)\n",
    "    rmse = torch.sqrt(torch.mean(torch.pow(preds.mean - y_test, 2)))\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].plot(hist['loss'])\n",
    "ax[0].set_title(\"Loss\")\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].scatter(y_test, preds.mean)\n",
    "ax[1].plot([-1, 1], [-1, 1], color='red', linestyle='--')\n",
    "ax[1].set_title(\"Predictions\")\n",
    "ax[1].set_xlabel(\"True\")\n",
    "ax[1].set_ylabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 12585\n"
     ]
    }
   ],
   "source": [
    "# KAN based model\n",
    "\n",
    "class KAN_extractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KAN_extractor, self).__init__()\n",
    "        self.kan1 = KANLayer(\n",
    "            in_dim=data_dim, out_dim=2*data_dim+1,\n",
    "            num=5, k=3, base_fun=torch.nn.SiLU())\n",
    "        self.kan2 = KANLayer(2*data_dim+1, 2,\n",
    "            num=5, k=3, base_fun=torch.nn.SiLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, *_ = self.kan1(x)\n",
    "        x, *_ = self.kan2(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "class KAN_GPR(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(KAN_GPR, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=2)),\n",
    "            num_dims=2, grid_size=50\n",
    "        )\n",
    "        self.feature_extractor = KAN_extractor()\n",
    "        self.scaler = gpytorch.utils.grid.ScaleToBounds(-1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.scaler(x)\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "kan_model = KAN_GPR(X_train, y_train, likelihood)\n",
    "\n",
    "# number of parameters\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in kan_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c988c9dd42914e9ab8aaba4708c103e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dangchan/miniforge3/envs/kan/lib/python3.9/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "step() missing 1 required positional argument: 'closure'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m         history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())  \u001b[38;5;66;03m# Save the loss value at each epoch\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n\u001b[0;32m---> 27\u001b[0m hist_kan \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[89], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, y_train)\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 22\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m iterator\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     24\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())  \u001b[38;5;66;03m# Save the loss value at each epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/kan/lib/python3.9/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/kan/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: step() missing 1 required positional argument: 'closure'"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "kan_model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = LBFGS(kan_model.parameters(), lr=0.1)\n",
    "\n",
    "# Loss\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, kan_model)\n",
    "\n",
    "def train():\n",
    "    history = {'loss': []}  # Create a dictionary to store the history\n",
    "    iterator = tqdm(range(epochs), desc=\"Epochs\")\n",
    "    for i in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        output = kan_model(X_train)\n",
    "        loss = -mll(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iterator.set_postfix(loss=loss.item())\n",
    "        history['loss'].append(loss.item())  # Save the loss value at each epoch\n",
    "    return history\n",
    "\n",
    "hist_kan = train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
